{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code from https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\n",
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein(\"I am Ge Gao, a cs student\", \"Ge Gao is a cs major student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07083333333333333\n",
      "0.05357142857142857\n",
      "0.35541310541310533\n",
      "0.11077626077626078\n",
      "0.06993589743589743\n",
      "0.0\n",
      "0.08810062486533073\n",
      "0.10714285714285714\n",
      "0.13333333333333333\n",
      "0.08035714285714285\n",
      "0.0\n",
      "0.07179487179487179\n",
      "0.06352941176470588\n",
      "0.0735042735042735\n",
      "0.09444444444444444\n",
      "0.07075320512820513\n",
      "0.08624708624708625\n",
      "0.048484848484848485\n",
      "0.14431818181818182\n",
      "0.0\n",
      "0.1527777777777778\n",
      "0.0\n",
      " \n",
      "0.17833333333333332\n",
      "0.13133903133903135\n",
      "0.06277056277056278\n",
      "0.38033395176252316\n",
      "0.11798941798941798\n",
      "0.0\n",
      "0.09711399711399711\n",
      "0.07875457875457875\n",
      "0.11388888888888889\n",
      "0.063003663003663\n",
      "0.0\n",
      "0.09023310023310023\n",
      "0.10958994708994708\n",
      "0.10833333333333334\n",
      "0.11527777777777778\n",
      "0.16111111111111112\n",
      "0.10328282828282828\n",
      "0.06947657605552343\n",
      "0.10793650793650796\n",
      "0.0\n",
      "0.15936507936507938\n",
      "0.0\n",
      " \n",
      "0.08920940170940171\n",
      "0.07766439909297052\n",
      "0.12460456210456211\n",
      "0.15124458874458874\n",
      "0.8701298701298701\n",
      "0.0\n",
      "0.15665584415584416\n",
      "0.07188375350140055\n",
      "0.08639455782312926\n",
      "0.0599031279178338\n",
      "0.0\n",
      "0.20334595959595958\n",
      "0.10492840492840493\n",
      "0.0837995337995338\n",
      "0.10205128205128204\n",
      "0.39312354312354314\n",
      "0.12785214785214785\n",
      "0.11361111111111111\n",
      "0.3373106060606061\n",
      "0.0\n",
      "0.11832611832611832\n",
      "0.0\n",
      " \n",
      "0.03333333333333333\n",
      "0.0606060606060606\n",
      "0.028846153846153848\n",
      "0.06378205128205128\n",
      "0.049950049950049945\n",
      "0.4\n",
      "0.046703296703296704\n",
      "0.02857142857142857\n",
      "0.04421768707482993\n",
      "0.023809523809523808\n",
      "0.0\n",
      "0.03675213675213675\n",
      "0.054761904761904755\n",
      "0.04\n",
      "0.04\n",
      "0.0707070707070707\n",
      "0.03636363636363636\n",
      "0.026666666666666665\n",
      "0.055708874458874465\n",
      "0.0\n",
      "0.0537037037037037\n",
      "0.0\n",
      " \n",
      "0.08989448051948053\n",
      "0.06190476190476189\n",
      "0.07252747252747252\n",
      "0.13472222222222224\n",
      "0.16305916305916301\n",
      "0.0\n",
      "1.0\n",
      "0.4664399092970522\n",
      "0.39070512820512826\n",
      "0.4081349206349207\n",
      "0.0\n",
      "0.4341386554621849\n",
      "0.0686965811965812\n",
      "0.07378335949764521\n",
      "0.09202226345083488\n",
      "0.07417582417582419\n",
      "0.09502164502164501\n",
      "0.05056689342403627\n",
      "0.11253468753468751\n",
      "0.0\n",
      "0.13149038461538462\n",
      "0.0\n",
      " \n",
      "0.08387445887445888\n",
      "0.059863945578231284\n",
      "0.06541895604395603\n",
      "0.13194444444444445\n",
      "0.1518243661100804\n",
      "0.0\n",
      "1.0\n",
      "0.43873015873015875\n",
      "0.3451923076923077\n",
      "0.36560846560846566\n",
      "0.0\n",
      "0.4002801120448179\n",
      "0.06987179487179487\n",
      "0.07362637362637361\n",
      "0.09064935064935065\n",
      "0.07417582417582418\n",
      "0.09818181818181818\n",
      "0.051269841269841264\n",
      "0.11144758019758019\n",
      "0.0\n",
      "0.1267094017094017\n",
      "0.0\n",
      " \n",
      "0.10212241462241461\n",
      "0.06551543694400837\n",
      "0.0712594696969697\n",
      "0.1296672077922078\n",
      "0.08486037771752057\n",
      "0.0\n",
      "0.315530303030303\n",
      "0.44743589743589746\n",
      "0.6106060606060606\n",
      "0.37286324786324787\n",
      "0.0\n",
      "0.08816738816738816\n",
      "0.07182757718472003\n",
      "0.08398268398268398\n",
      "0.11262626262626263\n",
      "0.0818903318903319\n",
      "0.10111111111111111\n",
      "0.05279866332497911\n",
      "0.11199633699633699\n",
      "0.0\n",
      "0.1478174603174603\n",
      "0.0\n",
      " \n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      " \n",
      "0.5723755411255411\n",
      "0.07031672031672032\n",
      "0.2550595238095238\n",
      "0.12476551226551227\n",
      "0.08412143412143412\n",
      "0.0\n",
      "0.4507575757575758\n",
      "0.06335192585192585\n",
      "0.08772169188835856\n",
      "0.05543293512043512\n",
      "0.2857142857142857\n",
      "0.08086484593837534\n",
      "0.0684015059015059\n",
      "0.07278911564625849\n",
      "0.09178519892805606\n",
      "0.08232600732600732\n",
      "0.08836996336996338\n",
      "0.04746811005103796\n",
      "0.09206099456099456\n",
      "0.0\n",
      "0.16264568764568765\n",
      "0.0\n",
      " \n",
      "0.078125\n",
      "0.06190476190476191\n",
      "0.04945054945054945\n",
      "0.11253561253561252\n",
      "0.18\n",
      "0.0\n",
      "0.37675070028011204\n",
      "0.06428571428571428\n",
      "0.07968253968253969\n",
      "0.04821428571428571\n",
      "0.0\n",
      "0.5631578947368421\n",
      "0.08326340326340327\n",
      "0.07211538461538462\n",
      "0.08750000000000001\n",
      "0.08012820512820512\n",
      "0.08021390374331551\n",
      "0.05833333333333333\n",
      "0.08639971139971142\n",
      "0.0\n",
      "0.10694444444444444\n",
      "0.0\n",
      " \n",
      "0.10714285714285714\n",
      "0.11298701298701297\n",
      "0.07777777777777778\n",
      "0.16388888888888892\n",
      "0.11259018759018757\n",
      "0.0\n",
      "0.09690170940170939\n",
      "0.08318181818181818\n",
      "0.10056689342403627\n",
      "0.06931818181818182\n",
      "0.0\n",
      "0.09107744107744109\n",
      "0.6121933621933622\n",
      "0.11047619047619048\n",
      "0.1619047619047619\n",
      "0.1162037037037037\n",
      "0.126007326007326\n",
      "0.07426470588235293\n",
      "0.1412202380952381\n",
      "0.0\n",
      "0.15252525252525254\n",
      "0.0\n",
      " \n",
      "0.06666666666666667\n",
      "0.06468531468531469\n",
      "0.0441025641025641\n",
      "0.09982905982905983\n",
      "0.08012820512820512\n",
      "0.0\n",
      "0.07362637362637361\n",
      "0.07142857142857142\n",
      "0.07738095238095238\n",
      "0.047619047619047616\n",
      "0.0\n",
      "0.0735042735042735\n",
      "0.08035714285714285\n",
      "1.0\n",
      "0.16666666666666666\n",
      "0.09230769230769231\n",
      "0.09090909090909091\n",
      "0.058823529411764705\n",
      "0.08398268398268396\n",
      "0.0\n",
      "0.1074074074074074\n",
      "0.0\n",
      " \n",
      "0.07388167388167388\n",
      "0.06147186147186147\n",
      "0.06298076923076923\n",
      "0.11369047619047618\n",
      "0.07665429808286951\n",
      "0.0\n",
      "0.30310470779220783\n",
      "0.4287081339712918\n",
      "0.3388506917918682\n",
      "0.35725677830940983\n",
      "0.0\n",
      "0.06838624338624338\n",
      "0.07559523809523809\n",
      "0.09428571428571428\n",
      "0.43151515151515146\n",
      "0.07554112554112553\n",
      "0.07916666666666666\n",
      "0.04795518207282913\n",
      "0.10585317460317462\n",
      "0.0\n",
      "0.11904761904761905\n",
      "0.0\n",
      " \n",
      "0.07799581371009942\n",
      "0.09048069985569987\n",
      "0.11296296296296296\n",
      "0.1099246432579766\n",
      "0.30847902097902097\n",
      "0.0\n",
      "0.07486217486217486\n",
      "0.06242838044308633\n",
      "0.07818362193362192\n",
      "0.053510040379788286\n",
      "0.0\n",
      "0.06991758241758242\n",
      "0.09722222222222221\n",
      "0.07670940170940171\n",
      "0.09099511599511599\n",
      "0.6035714285714285\n",
      "0.08154345654345654\n",
      "0.0981792717086835\n",
      "0.10886644219977552\n",
      "0.0\n",
      "0.10879200164914449\n",
      "0.0\n",
      " \n",
      "0.08333333333333333\n",
      "0.05952380952380952\n",
      "0.048863636363636366\n",
      "0.14136363636363639\n",
      "0.08254245754245754\n",
      "0.0\n",
      "0.09818181818181818\n",
      "0.08333333333333333\n",
      "0.09166666666666666\n",
      "0.05555555555555555\n",
      "0.0\n",
      "0.08624708624708625\n",
      "0.07435897435897436\n",
      "0.09090909090909091\n",
      "0.125\n",
      "0.08441558441558443\n",
      "1.0\n",
      "0.05555555555555555\n",
      "0.13205128205128205\n",
      "0.0\n",
      "0.1369047619047619\n",
      "0.0\n",
      " \n",
      "0.05128205128205129\n",
      "0.07638888888888888\n",
      "0.14583333333333331\n",
      "0.08455128205128205\n",
      "0.5364583333333334\n",
      "0.0\n",
      "0.06460084033613446\n",
      "0.058823529411764705\n",
      "0.06274509803921569\n",
      "0.0392156862745098\n",
      "0.0\n",
      "0.06018518518518518\n",
      "0.125\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n",
      "0.7000000000000001\n",
      "0.07142857142857142\n",
      "0.2\n",
      "0.1060504201680672\n",
      "0.0\n",
      "0.08119658119658119\n",
      "0.0\n",
      " \n",
      "0.0876068376068376\n",
      "0.07763532763532763\n",
      "0.17965367965367965\n",
      "0.15119047619047618\n",
      "0.3914141414141414\n",
      "0.0\n",
      "0.10992578849721708\n",
      "0.10696778711484593\n",
      "0.10721500721500721\n",
      "0.08557422969187675\n",
      "0.0\n",
      "0.08091491841491842\n",
      "0.10805860805860806\n",
      "0.08551864801864803\n",
      "0.10950854700854701\n",
      "0.4548484848484848\n",
      "0.14285714285714285\n",
      "0.127046783625731\n",
      "0.6394557823129252\n",
      "0.0\n",
      "0.17023809523809524\n",
      "0.0\n",
      " \n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      " \n",
      "0.1111111111111111\n",
      "0.07083333333333333\n",
      "0.09428571428571428\n",
      "0.17555555555555552\n",
      "0.09911616161616162\n",
      "0.0\n",
      "0.12427350427350428\n",
      "0.16666666666666666\n",
      "0.14583333333333331\n",
      "0.1111111111111111\n",
      "0.0\n",
      "0.10437710437710439\n",
      "0.08741258741258742\n",
      "0.1111111111111111\n",
      "0.16666666666666666\n",
      "0.10185185185185185\n",
      "0.14285714285714285\n",
      "0.0625\n",
      "0.18008658008658007\n",
      "0.0\n",
      "0.7222222222222223\n",
      "0.0\n",
      " \n",
      "0.06333333333333332\n",
      "0.05808080808080809\n",
      "0.04795204795204795\n",
      "0.09655225726654297\n",
      "0.07342657342657342\n",
      "0.0\n",
      "0.06823593073593073\n",
      "0.05482456140350876\n",
      "0.07091503267973856\n",
      "0.04385964912280701\n",
      "0.0\n",
      "0.06285936285936286\n",
      "0.07670940170940171\n",
      "0.07222222222222222\n",
      "0.1\n",
      "0.07272727272727272\n",
      "0.07118055555555555\n",
      "0.04722222222222222\n",
      "0.0865079365079365\n",
      "0.0\n",
      "0.09642857142857143\n",
      "0.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "from nltk import word_tokenize, pos_tag   ##implementation of similarity() from http://nlpforhackers.io/wordnet-sentence-similarity/\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "def pathsim(ss1, ss2):\n",
    "    if ss1.path_similarity(ss2) is None:\n",
    "        return 0\n",
    "    return ss1.path_similarity(ss2)\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    NoneType = type(None)\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence1nv = []\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    "    sentence2nv = []\n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = []\n",
    "    count1 = 0\n",
    "    count2=0\n",
    "    tup = ()\n",
    "    synsetnone1 = []\n",
    "    for tagged_word in sentence1:\n",
    "        if type(tagged_to_synset(*tagged_word))== NoneType and tagged_word[0][0].isupper() and penn_to_wn(tagged_word[1]) == 'n':\n",
    "            count1 += 1\n",
    "            tup = (tagged_word[0], tagged_word[1])\n",
    "            sentence1nv.append(tup)\n",
    "            synsetnone1.append(tup)\n",
    "            continue\n",
    "        if penn_to_wn(tagged_word[1]) == 'n':\n",
    "            count1 += 1\n",
    "            tup = (tagged_word[0], tagged_word[1])\n",
    "            sentence1nv.append(tup)\n",
    "    synsets2 = []\n",
    "    tup = ()\n",
    "    synsetnone2 = []\n",
    "    for tagged_word in sentence2:\n",
    "        if type(tagged_to_synset(*tagged_word))== NoneType and tagged_word[0][0].isupper() and penn_to_wn(tagged_word[1]) == 'n':\n",
    "            count2+=1\n",
    "            tup = (tagged_word[0], tagged_word[1])\n",
    "            sentence2nv.append(tup)\n",
    "            synsetnone2.append(tup)\n",
    "            continue\n",
    "        if penn_to_wn(tagged_word[1]) == 'n':\n",
    "            count2+=1\n",
    "            tup = (tagged_word[0], tagged_word[1])\n",
    "            sentence2nv.append(tup)\n",
    "    synsets1 = [tagged_to_synset(tagged_word, tag) for tagged_word, tag in sentence1nv]\n",
    "    synsets2 = [tagged_to_synset(tagged_word, tag) for tagged_word, tag in sentence2nv]\n",
    "    \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if not type(ss) == NoneType]\n",
    "    synsets2 = [ss for ss in synsets2 if not type(ss) == NoneType]\n",
    "    score = 0.0\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        if not synsets2 == []:\n",
    "            best_score = max([pathsim(synset, ss) for ss in synsets2])\n",
    "        # Check that the similarity could have been computed\n",
    "            if best_score is not None:\n",
    "                score += best_score\n",
    "    for synset in synsets2:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        if not synsets1 == []:\n",
    "            best_score = max([pathsim(synset, ss) for ss in synsets1])\n",
    "        # Check that the similarity could have been computed\n",
    "            if best_score is not None:\n",
    "                score += best_score\n",
    "    for synsetnone in synsetnone2:\n",
    "        if synsetnone in synsetnone1:\n",
    "            score += 1\n",
    "    for synsetnone in synsetnone1:\n",
    "        if synsetnone in synsetnone2:\n",
    "            score += 1\n",
    "    # Average the values\n",
    "    score /= (count1 + count2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Where can I find information about Sympa?\",\n",
    "    \"Where is Dr. Chan's office?\",\n",
    "    \"What is Sympa?\"\n",
    "    \"Can I view my online courses with my PC?\",\n",
    "    \"Can I tell other people my TRACKS pass code?\",\n",
    "    \"Do the computer labs offer free softwares?\",\n",
    "    \"How to install VPN\",\n",
    "    \"Can I forword Florida Tech emails to other email addresses?\",\n",
    "    \"How often should I check my email\",\n",
    "    \"What kind of files can be attached to my email?\",\n",
    "    \"How should I get emails from fitforum?\",\n",
    "    \"What is Sympa?\",\n",
    "    \"How do I find my Windows address?\",\n",
    "    \"How can I activate internet for my dorm room?\",\n",
    "    \"What is an ethernet card?\",\n",
    "    \"What should I do if items I scan always goes to 'Junk'\",\n",
    "    \"My computer is infected by viruses, what should I do?\",\n",
    "    \"How can I check my final grades?\",\n",
    "    \"What should I do if my laptop is stolen?\",\n",
    "    \"buying hardwares for University Use\",\n",
    "    \"How do I take a screenshot?\",\n",
    "    \"Can I make submissions to the on-line events?\",\n",
    "    \"Who can use cloud.fit.edu\"\n",
    "]\n",
    " \n",
    "focus_sentences = [\"How do I access my courses online?\",\n",
    "                   \"What should I do if someone asks me about my TRACKS password?\",\n",
    "                   \"What software is available in university computer labs?\",\n",
    "                   \"Installing Fortinet SSL VPN Client\",\n",
    "                   \"How do I forward emails from my Florida Tech email address to another email address?\",\n",
    "                   \"Do I have to check my Florida Tech email address?\",\n",
    "                   \"What is the maximum file size allowed for email attachments?\",\n",
    "                   \"How do I subscribe to fitforum?\",\n",
    "                   \"Information about Sympa, Florida Tech's List Server.\",\n",
    "                   \"How do I find my MAC address?\",\n",
    "                   \"When will my dorm room network connection be active?\",\n",
    "                   \"What is an ethernet card?\",\n",
    "                   \"How can I prevent items I scan to my email from going in the 'Junk' folder?\",\n",
    "                   \"What is the best way to protect my computer from viruses, hackers, and malware?\",\n",
    "                   \"How do I view my final grades?\",\n",
    "                   \"What should I do if my computer is stolen?\",\n",
    "                   \"Purchasing Computers for University Use\",\n",
    "                   \"How do I take a screenshot?\",\n",
    "                   \"How can I submit an on-campus event?\",\n",
    "                   \"How do I share folders with cloud.fit.edu\"]\n",
    "for focus_sentence in focus_sentences:\n",
    "    for sentence in sentences:\n",
    "        print(sentence_similarity(focus_sentence, sentence))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2888888888888889\n"
     ]
    }
   ],
   "source": [
    "print(sentence_similarity(\"dogs are awesome\", \"cats are beautiful animals\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Entity Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityChecking(s):\n",
    "    if \"Who\" in s or \"who\" in s:\n",
    "        print(\"Person.\")\n",
    "    if \"What\" in s or \"what\" in s:\n",
    "        print(\"Object\")\n",
    "    if \"When\" in s or \"when\" in s:\n",
    "        print(\"Time\")\n",
    "    if \"Where\" in s or \"where\" in s:\n",
    "        print(\"Place\")\n",
    "    if \"How\" in s or \"how\" in s:\n",
    "        print(\"How\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person.\n",
      "Object\n"
     ]
    }
   ],
   "source": [
    "entityChecking(\"What day is today? who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 1146068, 'geo-nam': 58388, 'org-nam': 48034, 'per-nam': 23790, 'gpe-nam': 20680, 'tim-dat': 12786, 'tim-dow': 11404, 'per-tit': 9800, 'per-fam': 8152, 'tim-yoc': 5290, 'tim-moy': 4262, 'per-giv': 2413, 'tim-clo': 891, 'art-nam': 866, 'eve-nam': 602, 'nat-nam': 300, 'tim-nam': 146, 'eve-ord': 107, 'org-leg': 60, 'per-ini': 60, 'per-ord': 38, 'tim-dom': 10, 'per-mid': 1, 'art-add': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCounter({u'O': 1146068, u'geo-nam': 58388, u'org-nam': 48034, u'per-nam': 23790, u'gpe-nam': 20680, u'tim-dat': 12786, u'tim-dow': 11404, u'per-tit': 9800, u'per-fam': 8152, u'tim-yoc': 5290, u'tim-moy': 4262, u'per-giv': 2413, u'tim-clo': 891, u'art-nam': 866, u'eve-nam': 602, u'nat-nam': 300, u'tim-nam': 146, u'eve-ord': 107, u'per-ini': 60, u'org-leg': 60, u'per-ord': 38, u'tim-dom': 10, u'per-mid': 1, u'art-add': 1})\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    " \n",
    "ner_tags = collections.Counter()\n",
    " \n",
    "corpus_root = \"gmb-2.2.0\"   # Make sure you set the proper path to the unzipped corpus\n",
    " \n",
    "for root, dirs, files in os.walk(corpus_root):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".tags\"):\n",
    "            with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                file_content = file_handle.read().decode('utf-8').strip()\n",
    "                annotated_sentences = file_content.split('\\n\\n')   # Split sentences\n",
    "                for annotated_sentence in annotated_sentences:\n",
    "                    annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]  # Split words\n",
    " \n",
    "                    standard_form_tokens = []\n",
    " \n",
    "                    for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                        annotations = annotated_token.split('\\t')   # Split annotations\n",
    "                        word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                        ner_tags[ner] += 1\n",
    "print(ner_tags)\n",
    "\"\"\"\n",
    "Counter({u'O': 1146068, u'geo-nam': 58388, u'org-nam': 48034, u'per-nam': 23790, u'gpe-nam': 20680, u'tim-dat': 12786, u'tim-dow': 11404, u'per-tit': 9800, u'per-fam': 8152, u'tim-yoc': 5290, u'tim-moy': 4262, u'per-giv': 2413, u'tim-clo': 891, u'art-nam': 866, u'eve-nam': 602, u'nat-nam': 300, u'tim-nam': 146, u'eve-ord': 107, u'per-ini': 60, u'org-leg': 60, u'per-ord': 38, u'tim-dom': 10, u'per-mid': 1, u'art-add': 1})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 1146068, 'geo': 58388, 'org': 48094, 'per': 44254, 'tim': 34789, 'gpe': 20680, 'art': 867, 'eve': 709, 'nat': 300})\n",
      "Words= 1354149\n"
     ]
    }
   ],
   "source": [
    "ner_tags = collections.Counter()\n",
    " \n",
    "for root, dirs, files in os.walk(corpus_root):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".tags\"):\n",
    "            with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                file_content = file_handle.read().decode('utf-8').strip()\n",
    "                annotated_sentences = file_content.split('\\n\\n')   # Split sentences\n",
    "                for annotated_sentence in annotated_sentences:\n",
    "                    annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]  # Split words\n",
    " \n",
    "                    standard_form_tokens = []\n",
    " \n",
    "                    for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                        annotations = annotated_token.split('\\t')   # Split annotation\n",
    "                        word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                        # Get only the primary category\n",
    "                        if ner != 'O':\n",
    "                            ner = ner.split('-')[0]\n",
    " \n",
    "                        ner_tags[ner] += 1\n",
    "print(ner_tags)\n",
    "# Counter({u'O': 1146068, u'geo': 58388, u'org': 48094, u'per': 44254, u'tim': 34789, u'gpe': 20680, u'art': 867, u'eve': 709, u'nat': 300})\n",
    " \n",
    "print(\"Words=\", sum(ner_tags.values()))\n",
    "# Words= 1354149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    " \n",
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[index - 1]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev-iob': previob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_conll_iob(annotated_sentence):\n",
    "    \"\"\"\n",
    "    `annotated_sentence` = list of triplets [(w1, t1, iob1), ...]\n",
    "    Transform a pseudo-IOB notation: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    to proper IOB notation: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    \"\"\"\n",
    "    proper_iob_tokens = []\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        tag, word, ner = annotated_token\n",
    " \n",
    "        if ner != 'O':\n",
    "            if idx == 0:\n",
    "                ner = \"B-\" + ner\n",
    "            elif annotated_sentence[idx - 1][2] == ner:\n",
    "                ner = \"I-\" + ner\n",
    "            else:\n",
    "                ner = \"B-\" + ner\n",
    "        proper_iob_tokens.append((tag, word, ner))\n",
    "    return proper_iob_tokens\n",
    " \n",
    "def read_gmb(corpus_root):\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".tags\"):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    " \n",
    "                        standard_form_tokens = []\n",
    " \n",
    "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                            annotations = annotated_token.split('\\t')\n",
    "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                            if ner != 'O':\n",
    "                                ner = ner.split('-')[0]\n",
    " \n",
    "                            if tag in ('LQU', 'RQU'):   # Make it NLTK compatible\n",
    "                                tag = \"``\"\n",
    " \n",
    "                            standard_form_tokens.append((word, tag, ner))\n",
    " \n",
    "                        conll_tokens = to_conll_iob(standard_form_tokens)\n",
    " \n",
    "                        # Make it NLTK Classifier compatible - [(w1, t1, iob1), ...] to [((w1, t1), iob1), ...]\n",
    "                        # Because the classfier expects a tuple as input, first item input, second the class\n",
    "                        yield [((w, t), iob) for w, t, iob in conll_tokens]\n",
    "reader = read_gmb(corpus_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Masked', 'VBN'), 'O'), (('assailants', 'NNS'), 'O'), (('with', 'IN'), 'O'), (('grenades', 'NNS'), 'O'), (('and', 'CC'), 'O'), (('automatic', 'JJ'), 'O'), (('weapons', 'NNS'), 'O'), (('attacked', 'VBD'), 'O'), (('a', 'DT'), 'O'), (('wedding', 'VBG'), 'O'), (('party', 'NN'), 'O'), (('in', 'IN'), 'O'), (('southeastern', 'JJ'), 'O'), (('Turkey', 'NNP'), 'B-geo'), ((',', ','), 'O'), (('killing', 'VBG'), 'O'), (('45', 'CD'), 'O'), (('people', 'NNS'), 'O'), (('and', 'CC'), 'O'), (('wounding', 'VBG'), 'O'), (('at', 'IN'), 'O'), (('least', 'JJS'), 'O'), (('six', 'CD'), 'O'), (('others', 'NNS'), 'O'), (('.', '.'), 'O')]\n",
      "------------\n",
      "[(('Turkish', 'JJ'), 'B-gpe'), (('officials', 'NNS'), 'O'), (('said', 'VBD'), 'O'), (('the', 'DT'), 'O'), (('attack', 'NN'), 'O'), (('occurred', 'VBD'), 'O'), (('Monday', 'NNP'), 'B-tim'), (('in', 'IN'), 'O'), (('the', 'DT'), 'O'), (('village', 'NN'), 'O'), (('of', 'IN'), 'O'), (('Bilge', 'NNP'), 'B-geo'), (('about', 'IN'), 'O'), (('600', 'CD'), 'O'), (('kilometers', 'NNS'), 'O'), (('from', 'IN'), 'O'), (('Ankara', 'NNP'), 'B-geo'), (('.', '.'), 'O')]\n",
      "------------\n",
      "[(('The', 'DT'), 'O'), (('wounded', 'VBN'), 'O'), (('were', 'VBD'), 'O'), (('taken', 'VBN'), 'O'), (('to', 'TO'), 'O'), (('the', 'DT'), 'O'), (('hospital', 'NN'), 'O'), (('in', 'IN'), 'O'), (('the', 'DT'), 'O'), (('nearby', 'JJ'), 'O'), (('city', 'NN'), 'O'), (('of', 'IN'), 'O'), (('Mardin', 'NNP'), 'B-geo'), (('.', '.'), 'O')]\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n[((u'They', u'PRP'), u'O'), ((u'marched', u'VBD'), u'O'), ((u'from', u'IN'), u'O'), ((u'the', u'DT'), u'O'), ((u'Houses', u'NNS'), u'O'), ((u'of', u'IN'), u'O'), ((u'Parliament', u'NN'), u'O'), ((u'to', u'TO'), u'O'), ((u'a', u'DT'), u'O'), ((u'rally', u'NN'), u'O'), ((u'in', u'IN'), u'O'), ((u'Hyde', u'NNP'), u'B-geo'), ((u'Park', u'NNP'), u'I-geo'), ((u'.', u'.'), u'O')]\\n------------\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(next(reader))\n",
    "print('------------')\n",
    "\"\"\"\n",
    "[((u'Thousands', u'NNS'), u'O'), ((u'of', u'IN'), u'O'), ((u'demonstrators', u'NNS'), u'O'), ((u'have', u'VBP'), u'O'), ((u'marched', u'VBN'), u'O'), ((u'through', u'IN'), u'O'), ((u'London', u'NNP'), u'B-geo'), ((u'to', u'TO'), u'O'), ((u'protest', u'VB'), u'O'), ((u'the', u'DT'), u'O'), ((u'war', u'NN'), u'O'), ((u'in', u'IN'), u'O'), ((u'Iraq', u'NNP'), u'B-geo'), ((u'and', u'CC'), u'O'), ((u'demand', u'VB'), u'O'), ((u'the', u'DT'), u'O'), ((u'withdrawal', u'NN'), u'O'), ((u'of', u'IN'), u'O'), ((u'British', u'JJ'), u'B-gpe'), ((u'troops', u'NNS'), u'O'), ((u'from', u'IN'), u'O'), ((u'that', u'DT'), u'O'), ((u'country', u'NN'), u'O'), ((u'.', u'.'), u'O')]\n",
    "------------\n",
    "\"\"\"\n",
    " \n",
    "print(next(reader))\n",
    "print('------------')\n",
    "\"\"\"\n",
    "[((u'Families', u'NNS'), u'O'), ((u'of', u'IN'), u'O'), ((u'soldiers', u'NNS'), u'O'), ((u'killed', u'VBN'), u'O'), ((u'in', u'IN'), u'O'), ((u'the', u'DT'), u'O'), ((u'conflict', u'NN'), u'O'), ((u'joined', u'VBD'), u'O'), ((u'the', u'DT'), u'O'), ((u'protesters', u'NNS'), u'O'), ((u'who', u'WP'), u'O'), ((u'carried', u'VBD'), u'O'), ((u'banners', u'NNS'), u'O'), ((u'with', u'IN'), u'O'), ((u'such', u'JJ'), u'O'), ((u'slogans', u'NNS'), u'O'), ((u'as', u'IN'), u'O'), ((u'\"', '``'), u'O'), ((u'Bush', u'NNP'), u'B-per'), ((u'Number', u'NN'), u'O'), ((u'One', u'CD'), u'O'), ((u'Terrorist', u'NN'), u'O'), ((u'\"', '``'), u'O'), ((u'and', u'CC'), u'O'), ((u'\"', '``'), u'O'), ((u'Stop', u'VB'), u'O'), ((u'the', u'DT'), u'O'), ((u'Bombings', u'NNS'), u'O'), ((u'.', u'.'), u'O'), ((u'\"', '``'), u'O')]\n",
    "------------\n",
    "\"\"\"\n",
    " \n",
    "print(next(reader))\n",
    "print('------------')\n",
    "\"\"\"\n",
    "[((u'They', u'PRP'), u'O'), ((u'marched', u'VBD'), u'O'), ((u'from', u'IN'), u'O'), ((u'the', u'DT'), u'O'), ((u'Houses', u'NNS'), u'O'), ((u'of', u'IN'), u'O'), ((u'Parliament', u'NN'), u'O'), ((u'to', u'TO'), u'O'), ((u'a', u'DT'), u'O'), ((u'rally', u'NN'), u'O'), ((u'in', u'IN'), u'O'), ((u'Hyde', u'NNP'), u'B-geo'), ((u'Park', u'NNP'), u'I-geo'), ((u'.', u'.'), u'O')]\n",
    "------------\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "from nltk.chunk.util import conlltags2tree\n",
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    " \n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 55809\n",
      "#test samples = 6201\n"
     ]
    }
   ],
   "source": [
    "reader = read_gmb(corpus_root)\n",
    "data = list(reader)\n",
    "training_samples = data[:int(len(data) * 0.9)]\n",
    "test_samples = data[int(len(data) * 0.9):]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples))    # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))                # test samples = 6201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = NamedEntityChunker(training_samples[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (gpe I/PRP)\n",
      "  'm/VBP\n",
      "  going/VBG\n",
      "  to/TO\n",
      "  (geo Germany/NNP)\n",
      "  this/DT\n",
      "  (tim Monday/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "print(chunker.parse(pos_tag(word_tokenize(\"I'm going to Germany this Monday.\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy import spatial\n",
    "import numpy\n",
    "def avg_feature_vector(words, model, num_features): ##implementation from https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt\n",
    "        #function to average all words vectors in a given paragraph\n",
    "        featureVec = numpy.zeros((num_features,), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "\n",
    "        #list containing names of words in the vocabulary\n",
    "        #index2word_set = set(model.index2word) this is moved as input param for performance reasons\n",
    "        for word in words:\n",
    "            if word in model.wv.index2word:\n",
    "                nwords = nwords+1\n",
    "                featureVec = numpy.add(featureVec, model.wv[word])\n",
    "        if(nwords>0):\n",
    "            featureVec = numpy.divide(featureVec, nwords)\n",
    "        \n",
    "        return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_1 = \"king john is fat\"\n",
    "sentence_1_avg_vector = avg_feature_vector(sentence_1.split(),model =  models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True), num_features=300)\n",
    "\n",
    "#get average vector for sentence 2\n",
    "sentence_2 = \"the queen eats a lot\"\n",
    "sentence_2_avg_vector = avg_feature_vector(sentence_2.split(), model =  models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True), num_features=300)\n",
    "sens = [sentence_1, sentence_2]\n",
    "\n",
    "if not sentence_1_avg_vector.all() * sentence_2_avg_vector.all() == 0:\n",
    "    sen1_sen2_similarity =  1 - spatial.distance.cosine(sentence_1_avg_vector,sentence_2_avg_vector)\n",
    "else:\n",
    "    sen1_sen2_similarity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"similarity: \")\n",
    "print(sen1_sen2_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy import spatial\n",
    "import numpy\n",
    "def avg_feature_vector(words, model, num_features): ##implementation from https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt\n",
    "        #function to average all words vectors in a given paragraph\n",
    "        featureVec = numpy.zeros((num_features,), dtype=\"float32\")\n",
    "        #list containing names of words in the vocabulary\n",
    "        #index2word_set = set(model.index2word) this is moved as input param for performance reasons\n",
    "        featureVec = model[words]\n",
    "        return featureVec\n",
    "sentence_1 = \"king\"\n",
    "sentence_1_avg_vector = avg_feature_vector(sentence_1,model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True), num_features=300)\n",
    "\n",
    "#get average vector for sentence 2\n",
    "sentence_2 = \"queen\"\n",
    "sentence_2_avg_vector = avg_feature_vector(sentence_2, model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True), num_features=300)\n",
    "sens = [sentence_1, sentence_2]\n",
    "\n",
    "if not sentence_1_avg_vector.all() * sentence_2_avg_vector.all() == 0:\n",
    "    sen1_sen2_similarity =  1 - spatial.distance.cosine(sentence_1_avg_vector,sentence_2_avg_vector)\n",
    "else:\n",
    "    sen1_sen2_similarity = 1\n",
    "print(\"similarity: \")\n",
    "print(sen1_sen2_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}